SlurmCtldHost=frank_nitty

ClusterName=cluster
SlurmUser=slurm
SlurmctldPort=6917-6996
SlurmdPort=6818
SrunPortRange=60001-63000

AuthType=auth/munge
AuthInfo=cred_expire=300
AuthAltTypes=auth/jwt
StateSaveLocation=/hpc/slurm/var/statesave
SlurmdSpoolDir=/var/spool/slurm
SwitchType=switch/none

MpiDefault=none
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/cgroup
#PluginDir=

# Refresh Group membership.
GroupUpdateForce=1
GroupUpdateTime=600

# Rebooting and Resuming and Cloud
ReturnToService=2
RebootProgram=/hotep/slurm/etc/scripts/node-reboot.sh
ResumeTimeout=900
# Power Saving
SuspendTime=-1 # this disables power saving
SuspendTimeout=30
#SuspendProgram=
#ResumeProgram=/hpc/slurm/etc/scripts/node-resume.sh
#ResumeFailProgram=/hpc/slurm/etc/scripts/node-resumefail.sh
#ResumeRate


# Job limits.
MaxArraySize=262144
MaxJobCount=1048576

# Allow jobs to run after reservation expires.
ResvOverRun=UNLIMITED

#PlugStackConfig=
#PropagatePrioProcess=
PropagateResourceLimits=NONE
PrologFlags=X11,Contain,Alloc,Serial
X11Parameters=home_xauthority

# Job submit plugins
JobSubmitPlugins=lua

# Epilog/Prolog parameters and scripts.

TaskPlugin=task/cgroup,task/affinity
UsePAM=1
TmpFS=/local/scratch

# TIMERS
SlurmctldTimeout=180
SlurmdTimeout=1200
BatchStartTimeout=30
MessageTimeout=60
InactiveLimit=0
EioTimeout=240
MinJobAge=300
KillWait=60
Waittime=0
UnkillableStepTimeout=240
# Priority

PriorityType=priority/multifactor
PriorityFlags=FAIR_TREE
PriorityDecayHalfLife=14-0
PriorityFavorSmall=NO
PriorityMaxAge=28-0
PriorityWeightAge=10000000
PriorityWeightFairshare=10000
PriorityWeightJobSize=100
PriorityWeightPartition=1000000
PriorityWeightQOS=2000000
PriorityUsageResetPeriod=MONTHLY

# LOGGING
SlurmctldDebug=debug
SlurmctldLogFile=/hpc/slurm/logs/slurmctld.log
SlurmctldSyslogDebug=verbose

SlurmdSyslogDebug=verbose

# Slurmd settings
SlurmdParameters=config_overrides

# ACCOUNTING
AccountingStorageHost=frank_nitty
JobAcctGatherType=jobacct_gather/cgroup

# NoShared and UsePSS are mutually exclusive, not sure yet which is best.
JobAcctGatherParams=NoOverMemoryKill,NoShared
#JobAcctGatherParams=NoOverMemoryKill,UsePSS

AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageUser=slurm
AccountingStorageEnforce=associations,limits,qos
AccountingStorageTRES=gres/gpu

SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory

# Scheduler
SchedulerType=sched/backfill
SchedulerParameters=enable_user_top
DependencyParameters=kill_invalid_depend
PreemptType=preempt/partition_prio
PreemptMode=REQUEUE
JobRequeue=1
LaunchParameters=disable_send_gids,use_interactive_step
InteractiveStepOptions="--interactive --preserve-env --pty $SHELL"
# OverTimeLimit=  # Set this per-partition.
EnforcePartLimits=ALL # Reject jobs that can't run with current limits.

# Generic resources types
GresTypes=gpu

# Mail settings
MailDomain=datahotep.com

HealthCheckProgram=/usr/sbin/nhc
HealthCheckInterval=300

# scrontab
ScronParameters=enable

Include nodes.conf
Include partitions.conf

